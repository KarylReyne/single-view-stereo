
\begin{frame}{New Dataset (1/4)}
    \begin{figure}[t]
        \includegraphics[width=.75\textwidth]{images/preview_image_01.png}
    \end{figure}
\end{frame}

\begin{frame}{New Dataset (2/4)}
    \begin{figure}[t]
        \includegraphics[width=.75\textwidth]{images/preview_image_02.png}
    \end{figure}
\end{frame}

\begin{frame}{New Dataset (3/4)}
    \begin{figure}[t]
        \includegraphics[width=.85\textwidth]{images/construction_image_01.png}
    \end{figure}
\end{frame}

\begin{frame}{New Dataset (4/4)}
    \begin{figure}[t]
        \includegraphics[width=.7\textwidth]{images/construction_image_02.png}
    \end{figure}
\end{frame}

\begin{frame}{New Architecture: StereoDiffusion \cite{Wang-2024-stereodiffusion}}
    \begin{figure}[t]
        \includegraphics[width=\textwidth]{images/StereoDiffusion_overview.png}
        \caption{From \cite{Wang-2024-stereodiffusion}}
    \end{figure}
\end{frame}

% \begin{frame}{StereoDiffusion: General Characteristics}
%     \begin{itemize}
%         \item Training-free: utilizes pretrained DPT \cite{ranftl2021visiontransformersdenseprediction} and StableDiffusion \cite{rombach2022highresolutionimagesynthesislatent} without fine-tuning
%         \item TODO
%     \end{itemize}
% \end{frame}

\begin{frame}{StereoDiffusion: Stereo Pixels Shift}
    The Stereo Pixels Shift operation is used to determine how much a pixel in the left image $x_{left}$ has to be shifted in the latent space to obtain the latent representation of the right image. It is calculated as follows:\\
    \begin{align}
        x_{right}(x,y)=x_{left}\lp{x-sD_{DPT}(x,y),y}
    \end{align}
    where $D_{DPT}$ is the disparity map estimated by DPT \cite{ranftl2021visiontransformersdenseprediction} and $s$ is a uniform scaling factor (see also \cite{Wang-2024-stereodiffusion}).
\end{frame}

\begin{frame}{StereoDiffusion: Stereo Pixels Shift With Sensor Data}
    We modify the Stereo Pixels Shift operation like so: 
    \begin{align}
        x_{right}(x,y)=x_{left}\lp{x-sD(x,y),y}\text{ with }D(x,y)=\frac{fB}{Z_{DPT}(x,y)}
    \end{align}
    where $Z_{DPT}$ is a depth map estimated by DPT and $f$ and $B$ are the focal length and baseline distance respectively.\\ 
    $f$ and $B$ are provided as additional input to the model, either by using the sensor data (Blender) from our dataset or by extracting them from a given prompt using \mono{Qwen2.5-7B-Instruct} \cite{an2024qwen2}.
\end{frame}

\begin{frame}{Qualitative Analysis: Input and Gold Output}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{figure}[t]
                \includegraphics[width=.75\textwidth]{images/Dino-Camera_011/left.jpg}
                \caption{Left stereo image}
            \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{figure}[t]
                \includegraphics[width=.75\textwidth]{images/Dino-Camera_011/right.jpg}
                \caption{Right stereo image}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Qualitative Analysis: Their Result}
    \begin{figure}[t]
        \includegraphics[width=.8\textwidth]{images/img2stereo/Dino-Camera_011/est_disp_B0.12_image_pair.png}
        \caption{Stereo images generated with StereoDiffusion + disparity map from DPT}
    \end{figure}
\end{frame}

\begin{frame}{Qualitative Analysis: $D_{DPT}$}
    \begin{figure}[t]
        \includegraphics[width=.4\textwidth]{images/img2stereo/Dino-Camera_011/est_disp_B0.12_DPT-disparity.png}
        \caption{The disparity map $D_{DPT}$ estimated by DPT}
    \end{figure}
\end{frame}

\begin{frame}{Qualitative Analysis: Our Result}
    \begin{figure}[t]
        \includegraphics[width=.8\textwidth]{images/img2stereo/Dino-Camera_011/est_depth_B0.12_image_pair.png}
        \caption{Stereo images generated with StereoDiffusion + depth map from DPT + sensor data}
    \end{figure}
\end{frame}

\begin{frame}{Qualitative Analysis: $Z_{DPT}$ and $D$}
    \begin{columns}
        \begin{column}{0.48\textwidth}
            \begin{figure}[t]
                \includegraphics[width=.75\textwidth]{images/img2stereo/Dino-Camera_011/est_depth_B0.12_DPT-depth.png}
                \caption{The depth map $Z_{DPT}$ estimated by DPT}
            \end{figure}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{figure}[t]
                \includegraphics[width=.75\textwidth]{images/img2stereo/Dino-Camera_011/est_depth_B0.12_DPT-depth-to-disparity.png}
                \caption{The disparity map $D$ calculated with sensor data based on $Z_{DPT}$}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Qualitative Analysis: Our Result with Modified Baseline Distance}
    \begin{figure}[t]
        \includegraphics[width=.73\textwidth]{images/img2stereo/Dino-Camera_011/est_depth_B0.24_image_pair.png}
        \caption{Stereo images generated with StereoDiffusion + depth map from DPT + prompted baseline + $f$ from sensor data}
    \end{figure}
\end{frame}

\begin{frame}{Qualitative Analysis: Our Result with Negative Baseline Distance}
    \begin{figure}[t]
        \includegraphics[width=.73\textwidth]{images/img2stereo/Dino-Camera_011/est_depth_B-0.24_image_pair.png}
        \caption{Stereo images generated with StereoDiffusion + depth map from DPT + prompted negative baseline + $f$ from sensor data}
    \end{figure}
\end{frame}

\begin{frame}{Qualitative Analysis: Their Result Deblurred}
    \begin{figure}[t]
        \includegraphics[width=.73\textwidth]{images/img2stereo/Dino-Camera_011/est_disp_deblur_B0.12_image_pair.png}
        \caption{Stereo images generated with StereoDiffusion + disparity map from DPT. The image is additionally deblurred using the algorithm provided with the StereoDiffusion source code.}
    \end{figure}
\end{frame}

% \begin{frame}{Possible Improvements}
%     \begin{itemize}
%         \item switch to a better/state-of-the-art monocular depth estimation model (see for example \cite{zhang2025surveymonocularmetricdepth,sun2025depthcondition})
%         \item TODO
%     \end{itemize}
% \end{frame}

\begin{frame}{Evaluation}
    \textbf{Goal:} Evaluate the \textbf{quality} of our generated views by comparing them to ground truth images\\
    \bigskip
    \textbf{Metrics:}
    \begin{itemize}
        \item \textbf{PSNR}(Peak Signal-to-Noise Ratio): \textbf{pixel-wise} and measure absolute intensity differences
        \item \textbf{SSIM}(Structural Similarity Index): compares contrast and structure, so checks if the \textbf{shapes and edges are consistent}
        \item \textbf{LPIPS}(Learned Perceptual Image Patch Similarity): Both images through pretrained neural network and evaluates perceptual similarity and \textbf{visual realism}
    \end{itemize}
\end{frame}

\begin{frame}{Related Work}
    \vspace{3mm}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{STNeRF (2024) -- Symmetry-Aware Triplane NeRF \cite{Liu-2025-stnerf}:}\\
            Enforces leftâ€“right symmetry (mainly for vehicles)\\
            \vspace{2mm}
            Pros:
            \begin{itemize}
                \item Very stable geometry
                \item Fewer artifacts
            \end{itemize}
            Cons:
            \begin{itemize}
                \item Domain-specific (vehicles only)
                \item Less general than our approach
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{NerfDiff (2023) -- Diffusion-to-NeRF Distillation \cite{gu2023nerfdiff}:}\\
            Distills a diffusion model into a NeRF representation \& generates consistent novel views from one image\\
            \vspace{2mm}
            Pros:
            \begin{itemize}
                \item Smoother views
                \item better view consistency 
            \end{itemize}
            Cons:
            \begin{itemize}
                \item High computational cost
                \item Limited geometric interpretability
            \end{itemize}
        \end{column}
    \end{columns}
    \begin{center}
        \textbf{Recent work improves visual quality using NeRF and diffusion priors, while our approach focuses on explicit geometry and stereo consistency}
    \end{center}
\end{frame}

\begin{frame}{Possible Improvement}
    Current approach uses SPSMD to handle occlusions while keeping the method training free\\
    \bigskip
    \textbf{Using Inpainting Model:}\\
    Are pretrained on large image datasets where random regions are masked and reconstructed.\\
    \bigskip
    Pros:
    \begin{itemize}
        \item Cleaner filling of large occluded regions
        \item Fewer artifacts
        \item Improved background texture quality
    \end{itemize}
\end{frame}

\begin{frame}{New Dataset: Credits}
    \begin{itemize}
        \item "Tyrannosaurus Rex Fossil Museum" (https://skfb.ly/pBEzK) by Chenchanchong is licensed under Creative Commons Attribution (http://creativecommons.org/licenses/by/4.0/).
        \item "Frazer Nash Le Mans" (https://skfb.ly/pDP6y) by Kryox Shade is licensed under Creative Commons Attribution (http://creativecommons.org/licenses/by/4.0/).
        \item ground texture from "The Mardou museum" (https://skfb.ly/pqxIz) by OuterspaceSoftware is licensed under Creative Commons Attribution-NonCommercial (http://creativecommons.org/licenses/by-nc/4.0/).
        \item "Discovery Hall" (https://skfb.ly/6UQZ8) by nickcramer is licensed under CC Attribution-NonCommercial-NoDerivs (http://creativecommons.org/licenses/by-nc-nd/4.0/).
        \item "Grand Piano" (https://skfb.ly/6UUnK) by Amatsukast is licensed under CC Attribution-NonCommercial-ShareAlike (http://creativecommons.org/licenses/by-nc-sa/4.0/).
        \item "Steam Train (Valley Railroad \#97)" (https://skfb.ly/6TPKL) by Kefla is licensed under CC Attribution-NonCommercial-NoDerivs (http://creativecommons.org/licenses/by-nc-nd/4.0/).
    \end{itemize}
\end{frame}